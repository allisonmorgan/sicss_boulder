{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install requests\n",
    "#!conda install beautifulsoup4\n",
    "#!conda install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Acknowledgements:</b> The code below is very much inspired by Chris Bail's [\"Screen-Scraping in R\"](https://cbail.github.io/SICSS_Screenscraping_in_R.html). Thanks Chris!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Trace Data: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) (also sometimes called \"screen-scraping\") is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems. For research questions where you need to visit many webpages, and collect essentially very similar information from each, web scraping can be a great tool.\n",
    "\n",
    "The typical web scraping program:\n",
    "<ol>\n",
    "    <li> Loads the address of a webpage to be scraped from your list of webpages</li>\n",
    "    <li> Downloads the HTML or XML of that website</li> \n",
    "    <li> Extracts any desired information</li>\n",
    "    <li> Saves that information in a convenient format (e.g. CSV, JSON, etc.)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day2-digital-trace-data/screenscraping/rmarkdown/Screen-Scraping.png\"></img>\n",
    "<em>From Chris Bail's \"Screen-Scraping in R\": <a href=\"https://cbail.github.io/SICSS_Screenscraping_in_R.html\">https://cbail.github.io/SICSS_Screenscraping_in_R.html</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legality & Politeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the internet was young, web scraping was a common and legally acceptable practice for collecting data on the web. But with the rise of online platforms, some of which rely heavily on user-created content (e.g. [Craigslist](https://en.wikipedia.org/wiki/Craigslist_Inc._v._3Taps_Inc.#Opinion_of_the_court)), the data made available on these sites has become recognized by their companies as highly valuable. Furthermore, from a website developer's perspective, web crawlers are able request many pages from your site in rapid succession, increasing server loads, and generally being a nuisance. \n",
    "\n",
    "Thus many websites, especially large sites (e.g. Yelp, AllRecipes, Instagram, The New York Times, etc.), have now forbidden \"crawlers\" / \"robots\" / \"spiders\" from harvesting their data in their \"Terms of Service\" (TOS). From Yelp's <a href=\"https://www.yelp.com/static?p=tos\">Terms of Service</a>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"terms_of_service.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before embarking on a research project that will involve web scraping, it is important to understand the TOS of the site you plan on collecting data from. \n",
    "\n",
    "If the site does allow web scraping (and you've consulted your legal professional), many websites have a [`robots.txt`](https://www.colorado.edu/robots.txt) file that tells search engines and web scrapers, written by researchers like you, how to interact with the site \"politely\" (i.e. the number of requests that can be made, pages to avoid, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting a Webpage in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an [HTML document](https://en.wikipedia.org/wiki/HTML) with [CSS](https://en.wikipedia.org/wiki/Cascading_Style_Sheets) and [Javascript](https://en.wikipedia.org/wiki/JavaScript) to produce a visually appealing page. For example, to us, the [Boulder Humane Society's listing of dogs available for adoption](https://www.boulderhumane.org/animals/adoption/dogs) looks something like what's displayed at the top of the browser below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boulder_dogs_html.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to your web browser, the page actually looks like the HTML source code (basically instructions for what text and images to show and how to do so) shown at the bottom of the page. To see the source code of a webpage, in Safari, go to `Develop > Show Page Source` or in Chrome, go to `Developer > View Source`.\n",
    "\n",
    "To request the HTML for a page in Python, you can use the Python package [`requests`](http://docs.python-requests.org/en/master/), as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "pet_pages = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "r = requests.get(pet_pages[0])\n",
    "html = r.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML. Notice how it's the same as the screenshot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded the HTML of the page, we next need to parse it. Let's say we want to extract all of the names, ages, and breeds of the dogs, cats, and small animals currently up for adoption at the Boulder Humane Society. \n",
    "\n",
    "Actually, navigating to the location of those attributes in the page can be somewhat tricky. Luckily HTML has a tree-structure, as shown below, where tags fit inside other tags. For example, the `title` of the document is located within its `head`, and within the larger `html` document (`<html> <head> <title> </title> ... </head>... </html>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day2-digital-trace-data/screenscraping/rmarkdown/html_tree.png\"></img>\n",
    "<em>From Chris Bail's \"Screen-Scraping in R\": <a href=\"https://cbail.github.io/SICSS_Screenscraping_in_R.html\">https://cbail.github.io/SICSS_Screenscraping_in_R.html</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the first pet on the page, we'll find that HTML element's \"CSS selector\". Within Safari, hover your mouse over the image of the first pet and then control+click on the image. This should highlight the section of HTML where the object you are trying to parse is found. Sometimes you may need to move your mouse through the HTML to find the exact location of the object (see [GIF](https://github.com/allisonmorgan/sicss_boulder/blob/master/day_2/copy_selector.gif))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"copy_selector.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can also go to 'Develop > Show Page Source' and then click 'Elements'. Hover your mouse over sections of the HTML until the object you are trying to find is highlighted within your browser.) \n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll pass the CSS selector that we just copied to BeautifulSoup to grab that object. Notice below how `select`-ing on that pet, shows us all of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"views-row views-row-1 views-row-odd views-row-first Adopt Me\">\n",
      "<div class=\"views-field views-field-field-pp-photo\"> <div class=\"field-content animal-photo\"><a href=\"/animals/adoption/38897934\"><img border=\"0\" height=\"162\" src=\"https://g.petango.com/photos/993/38fca025-4171-4f09-8593-6a083e90d3d1.jpg\" title=\"Adopt Me\" width=\"216\"/></a></div> </div>\n",
      "<div class=\"views-field views-field-field-pp-splashtitle\"> <div class=\"field-content\">Adopt Me</div> </div>\n",
      "<div class=\"views-field views-field-field-pp-animalname\"> <div class=\"field-content\"><a href=\"/animals/adoption/38897934\" title=\"Adopt Me!\">Max</a></div> </div>\n",
      "<div class=\"views-field views-field-field-pp-primarybreed\"> <div class=\"field-content\">Terrier, American Pit Bull</div> </div>\n",
      "<div class=\"views-field views-field-field-pp-secondarybreed\"> <div class=\"field-content\"></div> </div>\n",
      "<div class=\"views-field views-field-field-pp-age\"> <span class=\"views-label views-label-field-pp-age\">Age: </span> <span class=\"field-content\">6 years 11 months</span> </div>\n",
      "<div class=\"views-field views-field-field-pp-gender\"> <span class=\"views-label views-label-field-pp-gender\">Sex: </span> <span class=\"field-content\">Male</span> </div>\n",
      "<div class=\"views-field views-field-field-pp-animalid-1\"> <span class=\"views-label views-label-field-pp-animalid-1\">ID: </span> <span class=\"field-content\">38897934</span> </div>\n",
      "<div class=\"views-field views-field-edit-node\"> <span class=\"field-content\"></span> </div> </div>]\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "pet = soup.select(\"#block-system-main > div > div > div.view-content > div.views-row.views-row-1.views-row-odd.views-row-first.Adopt.Me\")\n",
    "print(pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can select the name, breeds, age, and gender of the pet by `find`-ing the `div` tags which contain this information. Notice how the `div` tag has the attribute (`attrs`) `class` with value \"views-field views-field-field-pp-animalname\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-animalname'})\n",
    "primary_breed = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-primarybreed'})\n",
    "secondary_breed = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-secondarybreed'})\n",
    "age = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Max', 'primary_breed': 'Terrier, American Pit Bull', 'secondary_breed': '', 'age': 'Age:6 years 11 months'}\n"
     ]
    }
   ],
   "source": [
    "# We can call `get_text()` on those objects to print them nicely.\n",
    "print({\n",
    "    \"name\": name.get_text(strip = True), \n",
    "    \"primary_breed\": primary_breed.get_text(strip = True), \n",
    "    \"secondary_breed\": secondary_breed.get_text(strip = True),\n",
    "    \"age\": age.get_text(strip=True)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get at the HTML object for each pet, we could find the CSS selector for each. Or, we can exploit the fact that every pet lives in a similar  HTML structure for each pet. That is, we can find all of the div tags with the class attribute which contain the string `views-row`. We'll print out their attributes like we just did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pets = soup.find_all('div', {'class': 'views-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Max', 'Terrier, American Pit Bull', '', 'Age:6 years 11 months']\n",
      "['Angel', 'Terrier, American Pit Bull', 'Mix', 'Age:3 years 3 months']\n",
      "['Bruno', 'Terrier, American Pit Bull', 'Mix', 'Age:6 years 6 months']\n",
      "['Zeus', 'Boxer', 'Mix', 'Age:2 years 1 month']\n",
      "['Pitkin', 'Retriever, Labrador', 'Mix', 'Age:3 years 0 months']\n",
      "['Penny', 'Australian Cattle Dog', 'Boxer', 'Age:7 years 0 months']\n",
      "['Cocoa', 'Bulldog, American', 'Mix', 'Age:0 years 3 months']\n",
      "['Mona', 'Rottweiler', 'Mix', 'Age:4 years 0 months']\n",
      "['Little Bit', 'Terrier, Jack Russell', 'Mix', 'Age:7 years 0 months']\n",
      "['Jefferson', 'Retriever, Chesapeake Bay', 'Mix', 'Age:4 years 0 months']\n",
      "['Linus', 'Terrier, Airedale', 'Mix', 'Age:8 years 0 months']\n",
      "['Willow Moon', 'Terrier, American Pit Bull', 'Border Collie', 'Age:1 year 3 months']\n",
      "['Kenna', 'Boxer', 'Mix', 'Age:7 years 4 months']\n",
      "['Reginald', 'Terrier, Rat', 'Mix', 'Age:3 years 0 months']\n",
      "['Christine', 'Chihuahua, Short Coat', 'Mix', 'Age:7 years 0 months']\n",
      "['Bubba', 'Siberian Husky', 'Mix', 'Age:3 years 6 months']\n",
      "['Kai', 'Australian Cattle Dog', 'Mix', 'Age:0 years 10 months']\n",
      "['Daisy Mae', 'Terrier, Rat', 'Mix', 'Age:1 year 6 months']\n",
      "['Vinnie', 'Terrier, American Pit Bull', 'Mix', 'Age:1 year 6 months']\n",
      "['Molly', 'Australian Shepherd', 'Mix', 'Age:0 years 6 months']\n",
      "['Bernice', 'Terrier, Jack Russell', 'Mix', 'Age:0 years 3 months']\n",
      "['Benita', 'Terrier, Jack Russell', 'Mix', 'Age:0 years 3 months']\n",
      "['Ziggy', 'Dalmatian', 'Mix', 'Age:1 year 3 months']\n",
      "['Daisy', 'Australian Cattle Dog', 'Mix', 'Age:1 year 6 months']\n",
      "['Keisha', 'Retriever, Labrador', 'Mix', 'Age:3 years 0 months']\n",
      "['Sophie', 'German Shepherd', 'Mix', 'Age:1 year 6 months']\n",
      "['Kodiak', 'Retriever, Labrador', 'Shepherd', 'Age:2 years 0 months']\n",
      "['Amber', 'Boxer', 'Mix', 'Age:0 years 7 months']\n",
      "['Nina', 'Boxer', 'Mix', 'Age:1 year 6 months']\n",
      "['Bellina', 'Schnauzer, Miniature', 'Mix', 'Age:1 year 0 months']\n"
     ]
    }
   ],
   "source": [
    "for pet in all_pets:\n",
    "    name = pet.find('div', {'class': 'views-field views-field-field-pp-animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': 'views-field views-field-field-pp-primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': 'views-field views-field-field-pp-secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': 'views-field views-field-field-pp-age'}).get_text(strip=True)\n",
    "    print([name, primary_breed, secondary_breed, age])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem like a fairly silly example of webscraping, but one could imagine several research questions using this data. For example, if we collected this data over time (e.g. using [Wayback Machine](https://archive.org/web/)), could we identify what features of pets -- names, breeds, ages -- make them more likely to be adopted? Are there certain names that are more common for certain breeds? Or maybe your research question is [something even wackier](https://qz.com/1025049/a-computer-used-artificial-intelligence-to-create-new-dog-names/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Read Tables from Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas](https://pandas.pydata.org) has really neat functionality in `read_html` where you can download an HTML table directly from a webpage, and load it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Image</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Often eaten with ketchup or brown sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacon, egg and cheese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Breakfast sandwich, usually with fried or scra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagel toast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Pressed, toasted bagel filled with vegetables ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baked bean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Canned baked beans on white or brown bread, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bánh mì[4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Filling is typically meat, but can contain a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbecue[5][6][7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Served on a bun, with chopped, sliced, or shre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Barros Jarpa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Ham and cheese, usually mantecoso, which is si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Barros Luco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Beef (usually thin-cut steak) and cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bauru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, roast beef, tomato, and pickled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beef on weck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States(Buffalo, New York)</td>\n",
       "      <td>Roast beef on a Kummelweck roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beirute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, sliced fresh tomatoes with oreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BLT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Named for its ingredients: bacon, lettuce, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bocadillo de calamares</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Baguette bread filled with fried squid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bologna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States, Canada</td>\n",
       "      <td>Pre-sliced and sometimes fried bologna sausage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boloney (Bologna) salad sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE Pennsylvania</td>\n",
       "      <td>A mixture of bologna sausage and sweet gherkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bosna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Usually grilled on white bread, containing a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bratwurst Sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>The Bratwurst sandwich is a popular street foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Breakfast roll</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom and Ireland</td>\n",
       "      <td>Convenience dish on a variety of bread rolls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Breakfast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Typically a scrambled or fried egg, cheese, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>British Rail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Reference to the poor quality of catering on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Name  Image                            Origin  \\\n",
       "0                              Bacon    NaN                    United Kingdom   \n",
       "1              Bacon, egg and cheese    NaN                     United States   \n",
       "2                        Bagel toast    NaN                            Israel   \n",
       "3                         Baked bean    NaN                     United States   \n",
       "4                         Bánh mì[4]    NaN                           Vietnam   \n",
       "5                  Barbecue[5][6][7]    NaN                     United States   \n",
       "6                       Barros Jarpa    NaN                             Chile   \n",
       "7                        Barros Luco    NaN                             Chile   \n",
       "8                              Bauru    NaN                            Brazil   \n",
       "9                       Beef on weck    NaN  United States(Buffalo, New York)   \n",
       "10                           Beirute    NaN                            Brazil   \n",
       "11                               BLT    NaN                     United States   \n",
       "12            Bocadillo de calamares    NaN                             Spain   \n",
       "13                           Bologna    NaN             United States, Canada   \n",
       "14  Boloney (Bologna) salad sandwich    NaN                   NE Pennsylvania   \n",
       "15                             Bosna    NaN                           Austria   \n",
       "16                Bratwurst Sandwich    NaN                           Germany   \n",
       "17                    Breakfast roll    NaN        United Kingdom and Ireland   \n",
       "18                         Breakfast    NaN                     United States   \n",
       "19                      British Rail    NaN                    United Kingdom   \n",
       "\n",
       "                                          Description  \n",
       "0             Often eaten with ketchup or brown sauce  \n",
       "1   Breakfast sandwich, usually with fried or scra...  \n",
       "2   Pressed, toasted bagel filled with vegetables ...  \n",
       "3   Canned baked beans on white or brown bread, so...  \n",
       "4   Filling is typically meat, but can contain a w...  \n",
       "5   Served on a bun, with chopped, sliced, or shre...  \n",
       "6   Ham and cheese, usually mantecoso, which is si...  \n",
       "7            Beef (usually thin-cut steak) and cheese  \n",
       "8   Melted cheese, roast beef, tomato, and pickled...  \n",
       "9                     Roast beef on a Kummelweck roll  \n",
       "10  Melted cheese, sliced fresh tomatoes with oreg...  \n",
       "11  Named for its ingredients: bacon, lettuce, and...  \n",
       "12             Baguette bread filled with fried squid  \n",
       "13  Pre-sliced and sometimes fried bologna sausage...  \n",
       "14  A mixture of bologna sausage and sweet gherkin...  \n",
       "15  Usually grilled on white bread, containing a b...  \n",
       "16  The Bratwurst sandwich is a popular street foo...  \n",
       "17  Convenience dish on a variety of bread rolls, ...  \n",
       "18  Typically a scrambled or fried egg, cheese, an...  \n",
       "19  Reference to the poor quality of catering on t...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_sandwiches\", header=0)[0]\n",
    "#table.to_csv(\"filenamehere.csv\") # Write table to CSV\n",
    "\n",
    "table.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting a Webpage with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our interactions with webpages involve rendering Javascript. For example, think of visiting a webpage with a search box, typing in a query, pressing search, and viewing the result. Or visiting a webpage that requires a login, or clicking through pages in a list. To handle pages like these we'll use a package in Python called [Selenium](http://selenium-python.readthedocs.io/index.html). (Installing Selenium can be a little tricky. You'll want to follow the directions as best you can [here](http://selenium-python.readthedocs.io/installation.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a fairly simple example: let's visit [xkcd](https://xkcd.com) and click through the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Safari() # This command opens a window in Safari\n",
    "# driver = selenium.webdriver.Chrome() # This command opens a window in Chrome\n",
    "\n",
    "# Get the xkcd website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eno's storied aria was once soloed by Judge Lance Ito on the alto oboe at Ohio's AirAsia Arena.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find an attribute of this page - the title of the comic.\n",
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue clicking throught the comics\n",
    "driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # Always remember to close your browser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now walk through how we can use Selenium to navigate the website to navigate a open source site called <a href=\"https://www.boxofficemojo.com\">\"Box Office Mojo\"</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"box_office_mojo.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Safari() # This command opens a window in Safari\n",
    "# driver = selenium.webdriver.Chrome() # This command opens a window in Chrome\n",
    "\n",
    "driver.get('https://www.boxofficemojo.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I wanted to know which movie was has been more lucrative 'Wonder Woman', 'Blank Panther', or 'Avengers: Infinity War'. I could type into the search bar on the upper left: 'Avengers: Infinity War'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in the search bar, and click 'Search'\n",
    "driver.find_element_by_xpath('//*[@id=\"leftnav\"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War')\n",
    "driver.find_element_by_xpath('//*[@id=\"leftnav\"]/li[2]/form/input[2]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can parse the table returned using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: An element could not be located on the page using the given search parameters.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8cf0f4b62148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"body\"]/table[2]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'innerHTML'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    964\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    965\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    322\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: An element could not be located on the page using the given search parameters.\n"
     ]
    }
   ],
   "source": [
    "table = driver.find_element_by_xpath('//*[@id=\"body\"]/table[2]')\n",
    "table.get_attribute('innerHTML').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_html(table.get_attribute('innerHTML').strip(), header=0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the link on the page and click it\n",
    "driver.find_element_by_xpath('//*[@id=\"body\"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the same for the remaining movies: 'Wonder Woman', and 'Black Panther' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # Always remember to close your browser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this might seem like a fairly simple example of web scraping, but there are some fun data science questions you could answer with this new technique. For example, for how many movie franchises, were their sequels more successful than their originals. For example, <a href=\"https://www.boxofficemojo.com/franchises/chart/?view=main&id=fastandthefurious.htm&sort=gross&order=DESC&p=.htm\">Furious 7</a> (see 'Adjusted for Ticket Price Inflation') was the most lucrative <a href=\"https://en.wikipedia.org/wiki/The_Fast_and_the_Furious\">Fast and the Furious</a> movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two examples hopefully show you how fun web scraping can be, but as I hinted at earlier in some cases web scraping is illegal. So when should you use this new tool? Here are some pointers:\n",
    "\n",
    "1. When it is not illegal.\n",
    "2. Ideally, when webpages you are trying to scrape share similar HTML structure.\n",
    "3. In cases where collecting the data by hand would be prohibitively expensive, in time or money.\n",
    "3. When the website you are trying to scrape has not made a publicly accessible API or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learned about how to scrape data from the web using Python's packages `requests`, `BeautifulSoup`, and `Selenium`. Some of you have already had experience with web scraping, but for others, this may have been your first time collecting digital trace data. \n",
    "\n",
    "This group exercise is designed to find a balance between practicing rudimentary skills (for those of you with little or no experience in this area) to cutting edge techniques (for those of you with extensive expertise in this area). As an added bonus, this exercise not only challenges you to practice your coding skills, but to think about how to ask questions that contribute new knowledge to sociological theory as well.\n",
    "\n",
    "<ol>\n",
    "<li>First, for five minutes, independently brainstorm one or two research questions that you believe can be answered using online data sources and web scraping. </li>\n",
    "<li>Divide yourselves into groups of three or four. Try to join a group with people you haven't worked with yet.</li>\n",
    "<li>For 10 minutes, work together to identify a research question based on one of the data sources proposed by your group members.</li>\n",
    "<li>Evaluate the strengths and weaknesses of the data you plan to collect.</li>\n",
    "<li>Outline a hybrid research design (e.g. an app or a bot) that could be used to address the weaknesses of the data you collected, or otherwise improve your ability to answer the research question.</li>\n",
    "<li>(If you have time, write code to collect data from each unit of analysis in your sample. See the code below for help.)</li>\n",
    "</ol>\n",
    "\n",
    "There is only one requirement: the group member with the least amount of experience coding should be responsible for typing the code into a computer. After 45 minutes, we will share our work with the group. Let us know if you'd like to present your group's potential project. Remember that these daily exercises are a way for you to explore new possible topics and to get to know each other better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Online Open Source Data & Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, is a short list of open source data and websites which may be useful for the group activity above. If you have any to add please tell Allie, or better yet, [start an issue or submit a pull request](https://github.com/allisonmorgan/sicss_boulder).\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\">Data is Plural\" Newsletter</a></li>\n",
    "    <li><a href=\"http://archive.ics.uci.edu/ml/index.php\">UCI Machine Learning Repository</a></li>\n",
    "    <li><a href=\"https://icon.colorado.edu/#!/\">University of Colorado Network Dataset</a></li> \n",
    "    <li><a href=\"https://snap.stanford.edu/data/\">Stanford Network Datasets</a></li>\n",
    "    <li><a href=\"https://networkdata.ics.uci.edu/resources.php\">UCI Network Datasets</a></li>\n",
    "    <li><a href=\"https://aws.amazon.com/datasets/8172056142375670\">Google Books nGrams</a></li>\n",
    "    <li><a href=\"http://about.reuters.com/researchandstandards/corpus/\">Reuters News Corpus</a></li>\n",
    "    <li><a href=\"http://www.nltk.org/nltk_data/\">NLTK Corpora</a></li>\n",
    "    <li><a href=\"https://www.cs.cmu.edu/~./enron/\">Enron Emails Dataset</a></li>\n",
    "    <li><a href=\"http://snap.stanford.edu/class/cs224w-2012/projects/cs224w-013-final.pdf\">Political Blogs 2008</a></li>\n",
    "    <li><a href=\"https://aws.amazon.com/public-data-sets/common-crawl/\">The Common Crawl</a></li>\n",
    "    <li><a href=\"https://meta.wikimedia.org/wiki/Data_dumps\">Wikipedia</a></li>\n",
    "    <li><a href=\"http://archive.ics.uci.edu/ml/datasets/Amazon+Commerce+reviews+set\">Amazon Commerce Reviews Set</a></li>\n",
    "    <li><a href=\"http://archive.ics.uci.edu/ml/datasets/NSF+Research+Award+Abstracts+1990-2003\">NSF Research Award Abstracts</a></li>\n",
    "    <li><a href=\"http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\">20 Newsgroups</a></li>\n",
    "    <li><a href=\"https://catalog.ldc.upenn.edu/LDC2008T19\">New York Times Annotated Corpus</a></li>\n",
    "    <li><a href=\"https://code.google.com/p/graphlabapi/downloads/detail?name=daily_kos.tar.bz2&can=2&q=\">Daily Kos Blog Posts</a></li>\n",
    "    <li><a href=\"http://deepdive.stanford.edu/doc/opendata/\">Stanford DeepDive Open Datasets</a></li>\n",
    "    <li><a href=\"http://trec.nist.gov/data/tweets/\">Tweets 2011</a></li>\n",
    "    <li><a href=\"http://www.boards.ie/\">Irish Discussion Boards</a></li>\n",
    "    <li><a href=\"http://www.cs.cornell.edu/people/pabo/movie-review-data/\">Movie Review Data</a></li>\n",
    "    <li><a href=\"https://www.yelp.com/academic_dataset\">Yelp Dataset</a></li>\n",
    "    <li><a href=\"http://www.biomedcentral.com/1471-2105/15/S11/S11\">BMC BioInformatics</a></li>\n",
    "    <li><a href=\"http://socialcomputing.asu.edu/datasets/Twitter\">ASU Twitter Dataset</a></li>\n",
    "    <li><a href=\"https://snap.stanford.edu/data/higgs-twitter.html\">Higgs Twitter Network Dataset</a></li>\n",
    "    <li><a href=\"http://icwsm.org/2013/datasets/datasets/\">ICWSM (Various Datasets)</a></li>\n",
    "    <li><a href=\"http://labrosa.ee.columbia.edu/millionsong/musixmatch\">Million Song Dataset</a></li>\n",
    "    <li><a href=\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XPCVEI\">EUSpeech</a></li>\n",
    "    <li><a href=\"http://humanrightstexts.org/\">Human Rights texts</a></li>\n",
    "    <li><a href=\"http://textlab.econ.columbia.edu/~jjacobs/marx/\">Marx Corpus</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
