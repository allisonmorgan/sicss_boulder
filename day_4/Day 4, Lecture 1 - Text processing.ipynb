{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style('whitegrid')\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of text processing\n",
    "\n",
    "Content in this section is adapted from Ramalho (2015) and Lutz (2013).\n",
    "\n",
    "The most basic characters in a string are the ASCII characters. The `string` library in Python, helpfully has these all listed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.ascii_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also punctionation and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string is basically a list of these mapping numbers. We can use some other functions and methods to analyze a string like we would with other iterables (like a list).\n",
    "\n",
    "The `len` of a string returns the number of characters in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('Brian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two (or more) strings can be combined by adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'Brian' + ' ' + 'Keegan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every character is mapped to an underlying integer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `chr` to do the reverse mapping: finding what character exists at a particular numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're doing comparisons, you're basically comparing these numbers to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'b' == 'B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first 128 characters. Some of these early characters aren't single characters, but are [control characters](https://en.wikipedia.org/wiki/Control_character) or [whitespace characters](https://en.wikipedia.org/wiki/Whitespace_character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,chr(i)) for i in range(128)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this ASCII character mapping doesn't include characters that have accents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Beyoncé'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last character `é` also exists at a specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('é')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the way that Python performs this mapping is not the same for computers everywhere else in the world. If we use the popular [UTF-8](https://en.wikipedia.org/wiki/UTF-8) standard to encode this string into generic byte-level representation, we get something interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = s.encode('utf8')\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of this `b` string somehow got a new character in it compared to the original `s` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s,len(s))\n",
    "print(b,len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to discover where these characters live and then map them back, we run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(b'\\xc3'), ord(b'\\xa9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(195), chr(169)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert from this byte-level representation back into Unicode with the `.decode` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(b'\\xc3\\xa9'.decode('utf8')), chr(233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different decoding standard like [CP1252](https://en.wikipedia.org/wiki/Windows-1252) returns something much more grotesque without throwing any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.decode('cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many, *many* kinds of [character encodings](https://en.wikipedia.org/wiki/Character_encoding) for representing non-ASCII text. \n",
    "\n",
    "![XKCD 927](https://imgs.xkcd.com/comics/standards.png) This cartoon pretty much explains why there are so many standards rather than a single standard:\n",
    "\n",
    "* [Latin-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1): the basis for many encodings\n",
    "* [CP-1252](https://en.wikipedia.org/wiki/Windows-1252): a common default encoding in Microsoft products similar to Latin-1\n",
    "* [UTF-8](https://en.wikipedia.org/wiki/UTF-8): one of the most widely adopted and compatible - *use it wherever possible*\n",
    "* [CP-437](https://en.wikipedia.org/wiki/Code_page_437): used by the original IBM PC (predates latin1) but this old zombie is still lurking\n",
    "* [GB-2312](https://en.wikipedia.org/wiki/GB_2312): implemented to support Chinese & Japanese characters, Greek & Cyrillic alphabets\n",
    "* [UTF-16](https://en.wikipedia.org/wiki/UTF-16): treats everyone equally poorly, here there also be emojis\n",
    "\n",
    "Other resources on why Unicode is what it is by [Ned Batchelder](https://nedbatchelder.com/text/unipain.html), this tutorial by [Esther Nam and Travis Fischer](https://www.slideshare.net/fischertrav/character-encoding-unicode-how-to-with-dignity-33352863), or [this Unicode tutorial](https://docs.python.org/3.5/howto/unicode.html) in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for codec in ['latin1','utf8','cp437','gb2312','utf16']:\n",
    "    print(codec.rjust(10),s.encode(codec), sep=' = ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will almost certainly encounter string encoding problems whenever you work with text data. Let's look at how quickly things can go wrong trying to decode a string when we don't know the standard. \n",
    "\n",
    "Some standards map the `\\xe9` byte-level representation to the `é` character we intended, while other standards have nothing at that byte location, and still others map that byte location to a different character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montreal_s = b'Montr\\xe9al'\n",
    "\n",
    "for codec in ['cp437','cp1252','latin1','gb2312','iso8859_7','koi8_r','utf8','utf16']:\n",
    "    print(codec.rjust(10),montreal_s.decode(codec,errors='replace'),sep=' = ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you discover the proper encoding given an observed byte sequence? **You can't.** But you can make some informed guesses by using a library like [`chardet`](https://github.com/chardet/chardet) to find clues based on relative frequencies and presence of byte-order marks.\n",
    "\n",
    "![](https://image.slidesharecdn.com/unicodeandcharacterencoding-140410003026-phpapp01/95/character-encoding-unicode-how-to-with-dignity-1-638.jpg?cb=1397089926)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your system's default? More like what *are* the defaults. The situation on PCs is generally a hot mess with Microsoft's standards like CP1252 competing with international standards like UTF-8, but Mac's generally try to keep everything in UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, locale\n",
    "\n",
    "expressions = \"\"\"\n",
    "locale.getpreferredencoding()\n",
    "my_file.encoding\n",
    "sys.stdout.encoding\n",
    "sys.stdin.encoding\n",
    "sys.stderr.encoding\n",
    "sys.getdefaultencoding()\n",
    "sys.getfilesystemencoding()\n",
    "\"\"\"\n",
    "\n",
    "my_file = open('dummy', 'w')\n",
    "\n",
    "for expression in expressions.split():\n",
    "    value = eval(expression)\n",
    "    print(expression.rjust(30), '=', repr(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you encounter problems with character encoding issues and you cannot discover the original encoding (utf8, latin1, cp1252 are always good ones to start with), you can try to ignore or replace the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montreal_s.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for error_handling in ['ignore','replace']:\n",
    "    print(error_handling,montreal_s.decode('utf8',errors=error_handling),sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortuantely, only tears, fist-shaking, and hair-pulling will give you the necessary experience to handle the inevitability of character encoding issues when working with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Wikipedia biographies of Presidents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from disk into memory. See Appendix 1 at the end of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('potus_wiki_bios.json','r') as f:\n",
    "    bios = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm there are 44 presidents (shaking fist at [Grover Cleveland](https://en.wikipedia.org/wiki/Grover_Cleveland), the 22nd *and* 24th POTUS) in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {0} biographies of presidents.\".format(len(bios)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's an example of a single biography? We access the dictionary by passing the key (President's name), which returns the value (the text of the biography)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = bios['Grover Cleveland']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to discuss how to process large text documents using athe [Natural Language Toolkit](http://www.nltk.org) library. \n",
    "​\n",
    "We first have to download some data corpora and libraries to use NLTK. Running this block of code *should* pop up a new window with four blue tabs: Collections, Corpora, Models, All Packages. Under Collections, Select the entry with \"book\" in the Identifier column and select download. Once the status \"Finished downloading collection 'book'.\" prints in the grey bar at the bottom, you can close this pop-up.\n",
    "​\n",
    "![](http://www.nltk.org/images/nltk-downloader.png)\n",
    "\n",
    "You should only need to do this next step once for each computer you're using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a specific lexicon for the sentiment analysis in the next lecture\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Opens the interface to download all the other corpora\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of processing natural language data is normalizing this data by removing variations in the text that the computer naively thinks are different entities but humans recognize as being the same. There are several steps to this including case adjustment (House to house), tokenizing (finding individual words), and stemming/lemmatization (\"tried\" to \"try\").\n",
    "\n",
    "This figure is a nice summary of the process of pre-processing your text data. The HTML to ASCII data step has already been done with the `get_page_content` function in the Appendix.\n",
    "\n",
    "![](http://www.nltk.org/images/pipeline1.png)\n",
    "\n",
    "In the case of case adjustment, it turns out several of the different \"words\" in the corpus are actually the same, but because they have different capitalizations, they're counted as different unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words\n",
    "How many words are in President Fillmore's article? \n",
    "\n",
    "A biography can be represented as a single large string (as it is now), but this huge string is not very helpful for analyzing features of the text until the string is segmented into \"tokens\", which include words but also hyphenated phrases or contractions (\"aren't\", \"doesn't\", *etc*.)\n",
    "\n",
    "There are a variety of different segmentation/tokenization strategies (with different tradeoffs) and corresponding methods implemented in NLTK.\n",
    "\n",
    "We could employ a naive approach of splitting on spaces. This turns out to create words out of happenstance punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ws_tokens = example.split(' ')\n",
    "print(\"There are {0:,} words when splitting on white spaces.\".format(len(example_ws_tokens)))\n",
    "example_ws_tokens[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use regular expressions to split on repeated whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_re_tokens = re.split(r'\\s+',example)\n",
    "print(\"There are {0:,} words when splitting on white spaces with regular expressions.\".format(len(example_re_tokens)))\n",
    "example_re_tokens[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear we want to separate words based on other punctuation as well so that \"Darkness,\" and \"Darkness\" aren't treated like separate words. Again, NLTK has a variety of methods for doing word tokenization more intelligently.\n",
    "\n",
    "`word_tokenize` is probably the easiest-to-recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_wt_tokens = nltk.word_tokenize(example)\n",
    "print(\"There are {0:,} words when splitting on white spaces with word_tokenize.\".format(len(example_wt_tokens)))\n",
    "example_wt_tokens[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are others like `wordpunct_tokenize` tha makes different assumptions about the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_wpt_tokens = nltk.wordpunct_tokenize(example)\n",
    "print(\"There are {0:,} words when splitting on white spaces with wordpunct_tokenize.\".format(len(example_wpt_tokens)))\n",
    "example_wpt_tokens[:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or `Toktok` is still another word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = nltk.ToktokTokenizer()\n",
    "example_ttt_tokens = toktok.tokenize(example)\n",
    "print(\"There are {0:,} words when splitting on white spaces with TokTok.\".format(len(example_ttt_tokens)))\n",
    "\n",
    "example_ttt_tokens[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of strategies for splitting a text document up into its constituent words, each making different assumptions about word boundaries, which results in different counts of the resulting tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,tokenlist in zip(['space_split','re_tokenizer','word_tokenizer','wordpunct_tokenizer','toktok_tokenizer'],[example_ws_tokens,example_re_tokens,example_wt_tokens,example_wpt_tokens,example_ttt_tokens]):\n",
    "    print(\"{0:>20}: {1:,} total tokens, {2:,} unique tokens\".format(name,len(tokenlist),len(set(tokenlist))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word cases\n",
    "Remember that strings of different cases (capitalizations) are treated as different words: \"young\" and \"Young\" are not the same. An important part of text processing is to remove un-needed variation, and mixed cases are variation we generally don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_wpt_lowered = [token.lower() for token in example_wpt_tokens]\n",
    "unique_wpt = len(set(example_wpt_tokens))\n",
    "unique_lowered_wpt = len(set(example_wpt_lowered))\n",
    "difference = unique_wpt - unique_lowered_wpt\n",
    "\n",
    "print(\"There are {0:,} unique words in example before lowering and {1:,} after lowering,\\na difference of {2} unique tokens.\".format(unique_wpt,unique_lowered_wpt,difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "English, like many languages, repeats many words in typical language that don't always convey a lot of information by themselves. When we do text processing, we should make sure to remove these \"stop words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(example_wpt_lowered).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK helpfully has a list of stopwords in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "english_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `string` module's \"punctuation\" attribute as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(string.punctuation)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine them so get a list of `all_stopwords` that we can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = english_stopwords + list(string.punctuation) + ['–']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a list comprehension to exclude the words in this stopword list from analysis while also gives each word similar cases. This is not perfect, but an improvement over what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt_lowered_no_stopwords = []\n",
    "\n",
    "for word in example_wpt_tokens:\n",
    "    if word.lower() not in all_stopwords:\n",
    "        wpt_lowered_no_stopwords.append(word.lower())\n",
    "\n",
    "fdist_wpt_lowered_no_stopwords = nltk.FreqDist(wpt_lowered_no_stopwords)\n",
    "fdist_wpt_lowered_no_stopwords.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of word frequencies, even after stripping out stopwords, follows a remarkable strong pattern. Most terms are used infrequently (upper-left) but a handful of terms are used repeatedly! [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) states:\n",
    "\n",
    "> \"the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counter = Counter(fdist_wpt_lowered_no_stopwords.values())\n",
    "\n",
    "f,ax = plt.subplots(1,1)\n",
    "\n",
    "ax.scatter(x=list(freq_counter.keys()),y=list(freq_counter.values()))\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Term frequency')\n",
    "ax.set_ylabel('Number of terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "[Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) (and the related concept of [stemming](https://en.wikipedia.org/wiki/Stemming)) are methods for dealing with conjugated words. Word like \"ate\" or \"eats\" are counted as distinct from \"eat\", although semantically they are similar and should likely be grouped together. Where stemming just removes commons suffixes and prefixes, sometimes resulting in mangled words, lemmatization attempts returns the root word. However, lemmatization can be extremely expensive computationally, which does not make it a good candidate for large corpora.\n",
    "\n",
    "The `get_wordnet_pos` and `lemmatizer` functions below work eith each other to lemmatize a word to its root. This involves attempting to discover the part-of-speech (POS) for each word and passing this POS to NLTK's lemmatize function, ultimately returning the root word (if it exists in the \"[wordnet](https://en.wikipedia.org/wiki/WordNet)\" corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatizer(token):\n",
    "    token,tb_pos = nltk.pos_tag([token])[0]\n",
    "    pos = get_wordnet_pos(tb_pos)\n",
    "    lemma = wnl.lemmatize(token,pos)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the tokens in `wpt_lowered_no_stopwords`, applying the `lemmatizer` function to each. Then inspect 25 examples of words where the lemmatizer changed the word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt_lemmatized = [lemmatizer(t) for t in wpt_lowered_no_stopwords]\n",
    "[(i,j) for (i,j) in list(zip(wpt_lowered_no_stopwords,wpt_lemmatized)) if len(i) != len(j)][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling the pieces all together\n",
    "\n",
    "We can combine all this functionality together into a single function `text_preprocessor` that takes a large string of text and returns a list of cleaned tokens, stripped of stopwords, lowered, and lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    \"\"\"Takes a large string (document) and returns a list of cleaned tokens\"\"\"\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    clean_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.lower() not in all_stopwords and len(t) > 2:\n",
    "            clean_tokens.append(lemmatizer(t.lower()))\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to every presidential biography (this may take a minute or so) and write the resulting list of cleaned tokens to the \"potus_wiki_bios_cleans.json\" file. We'll use this file in the next lecture as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each bio\n",
    "cleaned_bios = {}\n",
    "\n",
    "for bio_name,bio_text in bios.items():\n",
    "    cleaned_bios[bio_name] = text_preprocessor(bio_text)\n",
    "\n",
    "# Save to disk\n",
    "with open('potus_wiki_bios_cleaned.json','w') as f:\n",
    "    json.dump(cleaned_bios,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative descriptive statistics\n",
    "\n",
    "Now that we have cleaned biographies for each president, we can perform some basic analyses of the text. Which presidents have the longest biographies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_total_words = {}\n",
    "\n",
    "for bio_name,bio_text in cleaned_bios.items():\n",
    "    potus_total_words[bio_name] = len(bio_text)\n",
    "    \n",
    "pd.Series(potus_total_words).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_unique_words = {}\n",
    "\n",
    "for bio_name,bio_text in cleaned_bios.items():\n",
    "    potus_unique_words[bio_name] = len(set(bio_text))\n",
    "    \n",
    "pd.Series(potus_unique_words).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity) is the ratio of unique words to total words. Values closer to 0 indicate the presence of repeated words (low diversity) and values closer to 1 indicate words used only once (high diversity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(token_list):\n",
    "    unique_tokens = len(set(token_list))\n",
    "    total_tokens = len(token_list)\n",
    "    if total_tokens > 0:\n",
    "        return unique_tokens/total_tokens\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_lexical_diversity = {}\n",
    "\n",
    "for bio_name,bio_text in cleaned_bios.items():\n",
    "    potus_lexical_diversity[bio_name] = lexical_diversity(bio_text)\n",
    "    \n",
    "pd.Series(potus_lexical_diversity).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count how often a word occurs in each biography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Counter function\n",
    "from collections import Counter\n",
    "\n",
    "# Get counts of each token from the cleaned_bios for Grover Cleveland\n",
    "cleveland_counts = Counter(cleaned_bios['Grover Cleveland'])\n",
    "\n",
    "# Convert to a pandas Series and sort\n",
    "pd.Series(cleveland_counts).sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_word_counts = {}\n",
    "\n",
    "for bio_name,bio_text in cleaned_bios.items():\n",
    "    potus_word_counts[bio_name] = Counter(bio_text)\n",
    "    \n",
    "potus_word_counts_df = pd.DataFrame(potus_word_counts).T\n",
    "\n",
    "potus_word_counts_df.to_csv('potus_word_counts.csv',encoding='utf8')\n",
    "\n",
    "print(\"There are {0:,} unique words across the {1} presidents.\".format(potus_word_counts_df.shape[1],potus_word_counts_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words occur the most across presidential biographies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_word_counts_df.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Preprocess the S&P500 articles and compute statistics\n",
    "\n",
    "**Step 1**: Load the \"sp500_wiki_articles.json\", use the `text_preprocessor` function (or some other sequence of functions) from above to clean these articles up, and save the cleaned content to \"sp500_wiki_articles_cleaned.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Compute some descriptive statistics about the company articles with the most words, most unique words, greatest lexical diversity, most used words across articles, and number of unique words across all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Retrieving Wikipedia content by category\n",
    "\n",
    "Functions and operations to scrape the most recent (10 August 2018) Wikipedia content from every member of \"[Category:Presidents of the United States](https://en.wikipedia.org/wiki/Category:Presidents_of_the_United_States)\".\n",
    "\n",
    "The `get_page_content` function will get the content of the article as HTML and parse the HTML to return something close to a clean string of text. The `get_category_subcategories` and `get_category_members` will get all the members of a category in Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(title,lang='en',redirects=1):\n",
    "    \"\"\"Takes a page title and returns a (large) string of the HTML content \n",
    "    of the revision.\n",
    "    \n",
    "    title - a string for the title of the Wikipedia article\n",
    "    lang - a string (typically two letter ISO 639-1 code) for the language \n",
    "        edition, defaults to \"en\"\n",
    "    redirects - 1 or 0 for whether to follow page redirects, defaults to 1\n",
    "    parse - 1 or 0 for whether to return the raw HTML or paragraph text\n",
    "    \n",
    "    Returns:\n",
    "    str - a (large) string of the content of the revision\n",
    "    \"\"\"\n",
    "    \n",
    "    bad_titles = ['Special:','Wikipedia:','Help:','Template:','Category:','International Standard','Portal:','s:','File:','Digital object identifier','(page does not exist)']\n",
    "    \n",
    "    # Get the response from the API for a query\n",
    "    params = {'action':'parse',\n",
    "          'format':'json',\n",
    "          'page':title,\n",
    "          'redirects':redirects,\n",
    "          'prop':'text',\n",
    "          'disableeditsection':1,\n",
    "          'disabletoc':1\n",
    "         }\n",
    "\n",
    "    url = 'https://{0}.wikipedia.org/w/api.php'.format(lang)\n",
    "    req = requests.get(url,params=params)\n",
    "\n",
    "    json_string = json.loads(req.text)\n",
    "    \n",
    "    new_title = json_string['parse']['title']\n",
    "    \n",
    "    if 'parse' in json_string.keys():\n",
    "        page_html = json_string['parse']['text']['*']\n",
    "\n",
    "        # Parse the HTML into Beautiful Soup\n",
    "        soup = BeautifulSoup(page_html,'lxml')\n",
    "        \n",
    "        # Remove sections at end\n",
    "        bad_sections = ['See_also','Notes','References','Bibliography','External_links']\n",
    "        sections = soup.find_all('h2')\n",
    "        for section in sections:\n",
    "            if section.span['id'] in bad_sections:\n",
    "                \n",
    "                # Clean out the divs\n",
    "                div_siblings = section.find_next_siblings('div')\n",
    "                for sibling in div_siblings:\n",
    "                    sibling.clear()\n",
    "                    \n",
    "                # Clean out the ULs\n",
    "                ul_siblings = section.find_next_siblings('ul')\n",
    "                for sibling in ul_siblings:\n",
    "                    sibling.clear()\n",
    "        \n",
    "        # Get all the paragraphs\n",
    "        paras = soup.find_all('p')\n",
    "        \n",
    "        text_list = []\n",
    "        \n",
    "        for para in paras:\n",
    "            _s = para.text\n",
    "            # Remove the citations\n",
    "            _s = re.sub(r'\\[[0-9]+\\]','',_s)\n",
    "            text_list.append(_s)\n",
    "        \n",
    "        final_text = '\\n'.join(text_list).strip()\n",
    "        \n",
    "        return {new_title:final_text}\n",
    "\n",
    "def get_category_subcategories(category_title,lang='en'):\n",
    "    \"\"\"The function accepts a category_title and returns a list of the category's sub-categories\n",
    "    \n",
    "    category_title - a string (including \"Category:\" prefix) of the category name\n",
    "    lang - a string (typically two letter ISO 639-1 code) for the language edition,\n",
    "        defaults to \"en\"\n",
    "    \n",
    "    Returns:\n",
    "    members - a list containing strings of the sub-categories in the category\n",
    "    \n",
    "    \"\"\"\n",
    "    # Replace spaces with underscores\n",
    "    category_title = category_title.replace(' ','_')\n",
    "    \n",
    "    # Make sure \"Category:\" appears in the title\n",
    "    if 'Category:' not in category_title:\n",
    "        category_title = 'Category:' + category_title\n",
    "        \n",
    "    _S=\"https://{1}.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle={0}&cmtype=subcat&cmprop=title&cmlimit=500&format=json&formatversion=2\".format(category_title,lang)\n",
    "    json_response = requests.get(_S).json()\n",
    "\n",
    "    members = list()\n",
    "    \n",
    "    if 'categorymembers' in json_response['query']:\n",
    "        for member in json_response['query']['categorymembers']:\n",
    "            members.append(member['title'])\n",
    "            \n",
    "    return members\n",
    "    \n",
    "def get_category_members(category_title,depth=1,lang='en'):\n",
    "    \"\"\"The function accepts a category_title and returns a list of category members\n",
    "    \n",
    "    category_title - a string (including \"Category:\" prefix) of the category name\n",
    "    lang - a string (typically two letter ISO 639-1 code) for the language edition,\n",
    "        defaults to \"en\"\n",
    "    \n",
    "    Returns:\n",
    "    members - a list containing strings of the page titles in the category\n",
    "    \n",
    "    \"\"\"\n",
    "    # Replace spaces with underscores\n",
    "    category_title = category_title.replace(' ','_')\n",
    "    \n",
    "    # Make sure \"Category:\" appears in the title\n",
    "    if 'Category:' not in category_title:\n",
    "        category_title = 'Category:' + category_title\n",
    "    \n",
    "    _S=\"https://{1}.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle={0}&cmprop=title&cmnamespace=0&cmlimit=500&format=json&formatversion=2\".format(category_title,lang)\n",
    "    json_response = requests.get(_S).json()\n",
    "\n",
    "    members = list()\n",
    "    \n",
    "    if depth < 0:\n",
    "        return members\n",
    "    \n",
    "    if 'categorymembers' in json_response['query']:\n",
    "        for member in json_response['query']['categorymembers']:\n",
    "            members.append(member['title'])\n",
    "            \n",
    "    subcats = get_category_subcategories(category_title,lang=lang)\n",
    "    \n",
    "    for subcat in subcats:\n",
    "        members += get_category_members(subcat,depth-1)\n",
    "            \n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `get_category_members` to get all the immediate members (depth=0) of \"Category:Presidents of the United States.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = get_category_members('Presidents_of_the_United_States',depth=0)\n",
    "presidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the fourth through rest of the `presidents` list and get each president's biography using `get_page_content`. Store the results in the `presidents_wiki_bios` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents_wiki_bios = {}\n",
    "for potus in presidents[3:]:\n",
    "    presidents_wiki_bios.update(get_page_content(potus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('potus_wiki_bios.json','w') as f:\n",
    "    json.dump(presidents_wiki_bios,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2: Retrieving Wikipedia content from a list\n",
    "\n",
    "Wikipedia maintains a (superficially) up-to-date [List of S&P 500 companies](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies), but not a category of the constituent members. Like the presidents, we want to retrieve a list of all their Wikipedia articles, parse their content, and perform some NLP tasks.\n",
    "\n",
    "First, get the content of the article so we can parse out the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'List of S&P 500 companies'\n",
    "lang = 'en'\n",
    "redirects = 1\n",
    "\n",
    "params = {'action':'parse',\n",
    "          'format':'json',\n",
    "          'page':title,\n",
    "          'redirects':1,\n",
    "          'prop':'text',\n",
    "          'disableeditsection':1,\n",
    "          'disabletoc':1\n",
    "         }\n",
    "\n",
    "url = 'https://en.wikipedia.org/w/api.php'\n",
    "req = requests.get(url,params=params)\n",
    "\n",
    "json_string = json.loads(req.text)\n",
    "\n",
    "if 'parse' in json_string.keys():\n",
    "    page_html = json_string['parse']['text']['*']\n",
    "\n",
    "    # Parse the HTML into Beautiful Soup\n",
    "    soup = BeautifulSoup(page_html,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard way to get a list of the company names out is parsing the HTML table. We:\n",
    "\n",
    "1. Find all the tables in the `soup`\n",
    "2. Get the first table out\n",
    "3. Find all the rows in the table\n",
    "4. Loop through each row\n",
    "5. Find the links in each row\n",
    "6. Get the second link's title in each row\n",
    "7. Add the title to `company_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = []\n",
    "\n",
    "# Get the first table\n",
    "component_stock_table = soup.find_all('table')[0]\n",
    "\n",
    "# Get all the rows after the first (header) row\n",
    "rows = component_stock_table.find_all('tr')[1:]\n",
    "\n",
    "# Loop through each row and extract the title\n",
    "for row in rows:\n",
    "    # Get all the links in a row\n",
    "    links = row.find_all('a')\n",
    "    # Get the title in the 2nd cell from the left\n",
    "    title = links[1]['title']\n",
    "    # Add it to the company_links\n",
    "    company_names.append(title)\n",
    "\n",
    "print(\"There are {0:,} titles in the list\".format(len(set(company_names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easy eay is to use use pandas's [`read_html`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html) function to parse the table into a DataFrame and access the \"Security\" (second) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_html(str(component_stock_table),header=0)[0]\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = company_df['Security'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `get_page_content` to get the content of each company's page and add it to the `sp500_articles` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_articles = {}\n",
    "\n",
    "for company in set(company_links):\n",
    "    sp500_articles.update(get_page_content(company))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sp500_wiki_articles.json','w') as f:\n",
    "    json.dump(sp500_articles,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
