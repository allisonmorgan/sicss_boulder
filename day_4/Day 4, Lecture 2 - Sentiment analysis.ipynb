{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of sentiment analysis\n",
    "\n",
    "We will be using [VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and sEntiment Reasoner) for measuring sentiment, specifically in social media. This project has been integrated into the NLTK library, which we already have installed!\n",
    "\n",
    "The methodology the authors used:\n",
    "\n",
    "> Sentiment ratings from 10 independent human raters (all pre-screened, trained, and quality checked for optimal inter-rater reliability). Over 9,000 token features were rated on a scale from \"[–4] Extremely Negative\" to \"[4] Extremely Positive\", with allowance for \"[0] Neutral (or Neither, N/A)\". We kept every lexical feature that had a non-zero mean rating, and whose standard deviation was less than 2.5 as determined by the aggregate of those ten independent raters. This left us with just over 7,500 lexical features with validated valence scores that indicated both the sentiment polarity (positive/negative), and the sentiment intensity on a scale from –4 to +4. For example, the word \"okay\" has a positive valence of 0.9, \"good\" is 1.9, and \"great\" is 3.1, whereas \"horrible\" is –2.5, the frowning emoticon :( is –2.2, and \"sucks\" and it's slang derivative \"sux\" are both –1.5.\n",
    "\n",
    "We can access the lexicon that maps tokens to their average sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_df = pd.read_csv('https://github.com/cjhutto/vaderSentiment/raw/master/vaderSentiment/vader_lexicon.txt',sep='\\t',header=None)\n",
    "lexicon_df.columns = ['token','mean sentiment','std. dev.','human ratings']\n",
    "\n",
    "lexicon_dict = lexicon_df.set_index('token')['mean sentiment'].to_dict()\n",
    "\n",
    "lexicon_df.loc[1010:1020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional sentiment analysis tools like LIWC use a \"bag of words\" approach that is indifferent to word order, negations, etc. Let's use the word-level sentiment scores from VADER's lexicon, but use them in a super-naive \"bag of words\" approach. \n",
    "\n",
    "In the case of these two sentences, we tokenize the sentences into words, look up the words' sentiment scores in the `lexicon_dict`, print out each word's score, and then add up the scores for all the words in `sentiment_score_words1`. We see that this super-naive bag of words approach gives both of these sentences the same sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = wordpunct_tokenize(\"The book was good.\")\n",
    "words2 = wordpunct_tokenize(\"The book was not good.\")\n",
    "\n",
    "sentiment_score_words1 = 0 \n",
    "for w in words1:\n",
    "    lexicon_score = lexicon_dict.get(w.lower(),0)\n",
    "    print(w,lexicon_score)\n",
    "    sentiment_score_words1 += lexicon_score\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "sentiment_score_words2 = 0 \n",
    "for w in words2:\n",
    "    lexicon_score = lexicon_dict.get(w.lower(),0)\n",
    "    print(w,lexicon_score)\n",
    "    sentiment_score_words2 += lexicon_score\n",
    "\n",
    "print('\\n')\n",
    "print(\"The book was good. ==>\",sentiment_score_words1)\n",
    "print(\"The book was not good. ==>\",sentiment_score_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some sentences that have pretty clear valence for human readers, but computers are likely to struggle with. Extending the approach from above for these setnences, we can see how the super-naive bag of words approach. \n",
    "\n",
    "Easy phrases like \"The book was good\" are correctly classified as positive, but others like \"The book was not good.\" is incorrectly classified as positive, \"The book was not terrible\" is incorrectly classified as negative, \"The book was kind of good\" is incorrectly given a higher sentiment score than \"The book was good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The book was good.\",\n",
    "             \"The book was not good.\",\n",
    "             \"The book was not terrible.\", \n",
    "             \"The book was kind of good.\", \n",
    "             \"The book was good!\", \n",
    "             \"The book was good. :)\",\n",
    "             \"The book was good LOL\"\n",
    "            ]\n",
    "\n",
    "for s in sentences:\n",
    "    words = wordpunct_tokenize(s)\n",
    "    sentiment_score = 0\n",
    "    \n",
    "    for word in words:\n",
    "        lexicon_score = lexicon_dict.get(word.lower(),0)\n",
    "        sentiment_score += lexicon_score\n",
    "    \n",
    "    print(\"\\\"{0}\\\" ==> {1}\".format(s, str(sentiment_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to traditional \"bag of words\" approaches for sentiment analysis, VADER also preserves word order; includes emoticons (`:-)`), slang (`meh`), and initialisms (`lol`); and punctuation cues.\n",
    "\n",
    "We initialized the `analyzer` model from VADER's `SentimentIntensityAnalyzer` class we imported and pass a sentence to the `polarity_scores` method. VADER takes care of a lot of the tokenizing, casing, and stemming, so we can give it uncleaned sentences. \n",
    "\n",
    "The `polarity_scores` method returns a dictionary containing the metrics for the proportion of the text that are negative, neutral, and positive. The most important value for our purposes is the \"compound\" value with is the normalized sum of these valences: this is the sentiment score of a sentence where -1 is extremely negative and 1 is extremely positive.\n",
    "\n",
    "If we apply this to the two sentences, we can see VADER is much better at identifying the proper sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"The book was good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores(\"The book was not good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run VADER on all of the example `sentences` and see what their compound sentiment scores. Emoticons and initialisms greatly improve the sentiment! Negations like \"not\" and \"kind of\" appropriately reduce the valence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    scores = analyzer.polarity_scores(s)\n",
    "    print(\"\\\"{0}\\\" ==> {1}\".format(s, scores['compound']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis of presidential biographies\n",
    "\n",
    "Let's return to Wikipedia's presidential biographies and do some sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('potus_wiki_bios.json','r') as f:\n",
    "    bios = json.load(f)\n",
    "    \n",
    "print(\"There are {0} biographies of presidents.\".format(len(bios)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize each sentence in the biography. Let's do Grover Cleveland as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(bios['Grover Cleveland'])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using VADER's `polarity_scores` method on the 76th sentence in his biography, we see a \"compound\" score of 0.4767. This is a profoundly sad and negative sentiment but VADER still classified this as primarily positive. So sentiment analysis is still very far from perfect in many cases, but per the central limit theorem, aggregating many cases together should return some median tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[75])\n",
    "analyzer.polarity_scores(sentences[75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This double `for` loop goes through every president's biography, breaks the biography into sentences, and for every one of these sentences, computes a compound sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potus_sentiments = {}\n",
    "\n",
    "for president,bio in bios.items():\n",
    "    potus_sentiments[president] = []\n",
    "    sentences = sent_tokenize(bio)\n",
    "    for s in sentences:\n",
    "        polarity_scores = analyzer.polarity_scores(s)\n",
    "        compound_polarity = polarity_scores['compound']\n",
    "        potus_sentiments[president].append(compound_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a sentiment score for each presidential sentence, we can compute the average across all sentences within a biography, and sort the resulting average presidential sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_potus_sentiment = pd.Series({potus:np.mean(scores) for potus,scores in potus_sentiments.items()})\n",
    "mean_potus_sentiment.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distribution of sentence sentiments across biographies. Each line is a single president's distribution of sentence sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1)\n",
    "\n",
    "ax.axvline(x=0,c='k',lw=1,ls='--')\n",
    "ax.set_xlabel('Compound sentiment score')\n",
    "ax.set_title('Distribution of POTUS sentence sentiments')\n",
    "ax.set_xlim((-1.1,1.1))\n",
    "\n",
    "for potus,scores in potus_sentiments.items():\n",
    "    _s = pd.Series(scores)\n",
    "    _s.plot.kde(ax=ax,c='blue',alpha=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can cumulatively add up the sentiments sentence-by-sentence for each presidential biography. In other words, how does the cumulative sentiment of a presidential biography change over the course of an article (childhood to education to early professions to administration to post-POTUS life)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1)\n",
    "\n",
    "ax.set_xlabel('Sentence index')\n",
    "ax.set_ylabel('Cumulative compound sentiment')\n",
    "ax.set_title('POTUS sentiment through article')\n",
    "ax.axhline(y=0,c='k',lw=1,ls='--')\n",
    "\n",
    "for potus,scores in potus_sentiments.items():\n",
    "    #_s = pd.Series(np.cumsum(scores)/np.arange(1,len(scores)+1))\n",
    "    _s = pd.Series(np.cumsum(scores))\n",
    "    _s.plot(ax=ax,label=potus,c='blue',alpha=.25)\n",
    "\n",
    "#f.legend(ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({potus:np.sum(scores) for potus,scores in potus_sentiments.items()}).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis of Trump tweets\n",
    "\n",
    "The [Trump Twitter Archive](http://www.trumptwitterarchive.com/archive) maintains an up-to-date archive of all the tweets from [@realDonaldTrump](https://twitter.com/realDonaldTrump). I broke Twitter's TOS and screen-scaped all of this historical tweets (but not retweets) generated by this account as well as for Hillary Clinton, Mitt Romney, and Barack Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_tweets_trump.json','r') as f:\n",
    "    trump_tweets = json.load(f)\n",
    "    \n",
    "with open('cleaned_tweets_clinton.json','r') as f:\n",
    "    clinton_tweets = json.load(f)\n",
    "    \n",
    "with open('cleaned_tweets_romney.json','r') as f:\n",
    "    romney_tweets = json.load(f)\n",
    "    \n",
    "with open('cleaned_tweets_obama.json','r') as f:\n",
    "    obama_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a single tweet (these aren't in chronological order... yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this (flat) JSON data to a pandas DataFrame so it's easier to manipulate and visualize. We'll also use this `new_tweet_features` function to generate some new features (date, year, hour, weekday, word count) that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tweet_features(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['weekday'] = df['date'].apply(lambda x:x.weekday())\n",
    "    df['year'] = df['date'].apply(lambda x:x.year)\n",
    "    df['hour'] = df['timestamp'].apply(lambda x:x.hour)\n",
    "    df['wordcount'] = df['text'].apply(lambda x:len(wordpunct_tokenize(x)))\n",
    "    df = df.sort_values('timestamp')\n",
    "    return df\n",
    "\n",
    "trump_tweets_df = pd.DataFrame(trump_tweets)\n",
    "trump_tweets_df = new_tweet_features(trump_tweets_df)\n",
    "trump_tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well-known [blogpost by David Robinson](http://varianceexplained.org/r/trump-tweets/) explored the \"source\" field in a tweet as a valuable instrument for differentiating between the people communicating through the \"realDonaldTrump\" account: \"Android tweets are angrier and more negative.\" \n",
    "\n",
    "[President Trump adopted an iPhone in 2017](https://twitter.com/DanScavino/status/846918912793083904).\n",
    "\n",
    "We can make a plot by year. We'll facet (using \"hue\") by the source of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sources = ['Twitter for iPhone','Twitter for Android','Twitter Web Client']\n",
    "sb.catplot(x = 'year',\n",
    "           y = 'sentiment',\n",
    "           hue = 'source',\n",
    "           data = trump_tweets_df,\n",
    "           hue_order = top_sources,\n",
    "           kind = 'point',\n",
    "           aspect = 2,\n",
    "           dodge = .25\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a plot by hour of the day. We'll facet (using \"hue\") by the source of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x = 'hour',\n",
    "           y = 'sentiment',\n",
    "           hue = 'source',\n",
    "           data = trump_tweets_df,\n",
    "           hue_order = top_sources,\n",
    "           kind = 'point',\n",
    "           aspect = 2,\n",
    "           dodge = .25\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a plot by hour of the day. We'll facet (using \"hue\") by tweets from different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x = 'hour',\n",
    "           y = 'sentiment',\n",
    "           hue = 'year',\n",
    "           data = trump_tweets_df,\n",
    "           hue_order = [2015,2016,2017,2018],\n",
    "           kind = 'point',\n",
    "           aspect = 2,\n",
    "           dodge = .25\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of sentiment by day of the week (0 = Monday, 6 = Sunday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(x = 'weekday',\n",
    "           y = 'sentiment',\n",
    "           hue = 'year',\n",
    "           data = trump_tweets_df,\n",
    "           hue_order = [2015,2016,2017,2018],\n",
    "           kind = 'point',\n",
    "           aspect = 2,\n",
    "           dodge = .25\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a crosstab on the data to get a count of tweets by hour of day and day of week since he started his 2016 campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean index to only get rows of data after June 16, 2015\n",
    "pres_trump_tweets = trump_tweets_df[trump_tweets_df['timestamp'] > pd.Timestamp('2015-06-16')]\n",
    "\n",
    "# Cross-tab by hour and weekday\n",
    "ct_count = pd.crosstab(index=pres_trump_tweets['hour'],\n",
    "                       columns=pres_trump_tweets['weekday'],\n",
    "                      )\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(7,12))\n",
    "sb.heatmap(ct_count,ax=ax,cmap='rainbow',square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also plot a heatmap of Trump's Twitter sentiment by weekday and hour since he started his campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab by hour and weekday, but values are average sentiment (not counts from before)\n",
    "ct_sentiment = pd.crosstab(index=pres_trump_tweets['hour'],\n",
    "                           columns=pres_trump_tweets['weekday'],\n",
    "                           values=pres_trump_tweets['sentiment'],\n",
    "                           aggfunc=np.mean\n",
    "                          )\n",
    "\n",
    "# Plot the data\n",
    "f,ax = plt.subplots(1,1,figsize=(7,12))\n",
    "sb.heatmap(ct_sentiment,ax=ax,cmap='rainbow_r',square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing daily approval with daily sentiment\n",
    "\n",
    "How has President Trump's Twitter sentiment score varied with his approval number among voters? We grab the daily averaged approval data from FiveThirtyEight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_approvals = pd.read_csv('https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv',parse_dates=['modeldate'])\n",
    "fte_voters = fte_approvals[fte_approvals['subgroup'] == 'Voters']\n",
    "fte_approvals.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the approval rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "fte_voters.plot.line(x='modeldate',y='approve_estimate',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trump, like other Twitter users, can tweet multiple times in a day. We will perform a groupby-aggregation operation to compute the average sentiment of all tweets per day.\n",
    "\n",
    "I've plotted the 90-day rolling average of sentiment and indicated five major events:\n",
    "\n",
    "* The 2011 White House Correspondent's Dinner where Trump was roasted by Obama and Seth Meyers\n",
    "* Trump's 2016 campaign launched in June 2015\n",
    "* Trump unexpectedly won the presidency in November 16\n",
    "* Trump was inaugurated in January 2017\n",
    "* The Department of Justice appointed a Special Investigator in May 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trump_sentiment = trump_tweets_df.groupby('date').agg({'sentiment':np.mean})['sentiment']\n",
    "daily_trump_sentiment = daily_trump_sentiment.reindex(pd.date_range(daily_trump_sentiment.index.min(),daily_trump_sentiment.index.max()),fill_value=0)\n",
    "\n",
    "f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "daily_trump_sentiment.rolling(90).mean().plot(ax=ax,lw=3)\n",
    "\n",
    "ax.set_ylim((-.1,.4))\n",
    "ax.axhline(y=0,c='k',ls='--')\n",
    "ax.axvline(x=pd.Timestamp('2011-05-01'),c='k') # WH Correspondent's Dinner\n",
    "ax.axvline(x=pd.Timestamp('2015-06-16'),c='k') # Campaign launched\n",
    "ax.axvline(x=pd.Timestamp('2016-11-08'),c='k') # Election\n",
    "ax.axvline(x=pd.Timestamp('2017-01-20'),c='k') # Inauguration\n",
    "ax.axvline(x=pd.Timestamp('2017-05-17'),c='k') # Special Investigator appointed\n",
    "ax.set_ylabel('Mean sentiment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've made an `approval_sentiment_df` DataFrame by combining FiveThirtyEight and sentiment scores we computed. We can plot the relationship between these values using seaborn's `lmplot` with LOWESS and print the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_approval = fte_voters.set_index('modeldate')['approve_estimate']\n",
    "trump_sentiment = daily_trump_sentiment.loc[fte_approval.index]\n",
    "approval_sentiment_df = pd.DataFrame({'Approval':trump_approval,'Sentiment':trump_sentiment})\n",
    "\n",
    "corr = approval_sentiment_df.corr().loc['Approval','Sentiment']\n",
    "print('Correlation is: {0:.4f}'.format(corr))\n",
    "\n",
    "sb.lmplot(x = 'Approval',\n",
    "          y = 'Sentiment',\n",
    "          data = approval_sentiment_df,\n",
    "          lowess = True,\n",
    "          line_kws = {'lw':3,'color':'red'},\n",
    "          aspect = 2\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(12,4))\n",
    "\n",
    "approval_sentiment_df.rolling(7).mean().plot(secondary_y='Sentiment',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Exploring other presidential candidates' Twitter histories\n",
    "\n",
    "**Step 1**: Convert Obama (`obama_tweets`), Clinton (`clinton_tweets`), or Romney's (`romney_tweets`) tweets to a pandas DataFrame and use the `new_tweet_features` function to add some new variables to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Use seaborn's [`catplot`](https://seaborn.pydata.org/generated/seaborn.catplot.html) to visualize some features of these candidates' Twitter histories (by year, by day of week, by hour of day, by device, *etc*.). How do these candidates' activity compare to Trump's Twitter activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Plot the moving average of other candidates' sentiment over time using a group-aggregation, reindexing, and rolling average like we did with Trump above. What are some key changepoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Get complete user timelines and hydrate their tweets\n",
    "\n",
    "Here is some code called [GetOldTweets](https://github.com/Jefferson-Henrique/GetOldTweets-python) with some complex dependencies that flagrantly violates Twitter's Terms of Service in order to get all of the tweets from a user's timeline (rather than the 3,200 most recent). Twitter will eventually block your IP address if you run this code too much. \n",
    "\n",
    "I am comfortable violating Twitter's ToS in order to retrieve an archive of presidential candidates' tweets that is not otherwise available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetOldTweets import got3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def tweet_converter(tweet):\n",
    "    \"\"\"\n",
    "    Takes a tweet object generated by the GetOldTweets getTweets() function\n",
    "    and converts it to a dictionary\n",
    "    \"\"\"\n",
    "    d = {'author_id': tweet.author_id,\n",
    "         'favorites': tweet.favorites, \n",
    "         'formatted_date': tweet.formatted_date, \n",
    "         'location': tweet.geo, \n",
    "         'hashtags': tweet.hashtags, \n",
    "         'id': tweet.id, \n",
    "         'mentions': tweet.mentions, \n",
    "         'permalink': tweet.permalink,\n",
    "         'retweets': tweet.retweets, \n",
    "         'text': tweet.text, \n",
    "         'urls': tweet.urls, \n",
    "         'username': tweet.username}\n",
    "    return d \n",
    "\n",
    "def tweet_grabber(screen_name,year,write_json=False):\n",
    "    \"\"\"\n",
    "    Takes a screen_name as a string and a year and pulls all the tweets and replies from that account in that year\n",
    "    Writes the tweets as two separate JSON files in the format 'tweets_<screenname>_<year>.json' \n",
    "    and 'replies_<screenname>_<year>.json'\n",
    "    \n",
    "    Returns two dictionaries, containing the tweets and replies\n",
    "    \"\"\"\n",
    "    year = int(year)\n",
    "    \n",
    "    # Get the tweets for a single year and convert the tweet objects to a dictionary\n",
    "    tweet_criteria = got3.manager.TweetCriteria().setUsername(screen_name).setSince(\"{0}-01-01\".format(year)).setUntil(\"{0}-01-01\".format(year+1)).setMaxTweets(9999)\n",
    "    tweets = got3.manager.TweetManager.getTweets(tweet_criteria)\n",
    "    converted_tweets = [tweet_converter(t) for t in tweets]\n",
    "    \n",
    "    # Get the replies for a single year and convert the tweet objects to a dictionary\n",
    "    replies_criteria = got3.manager.TweetCriteria().setUsername(screen_name).setSince(\"{0}-01-01\".format(year)).setUntil(\"{0}-01-01\".format(year+1)).setQuerySearch('filter:replies').setMaxTweets(9999)\n",
    "    replies = got3.manager.TweetManager.getTweets(replies_criteria)\n",
    "    converted_replies = [tweet_converter(t) for t in replies]\n",
    "    \n",
    "    # Only write to disk at this step if write_json is True\n",
    "    if write_json:\n",
    "        # Save to disk\n",
    "        with open('tweets_{0}_{1}.json'.format(screen_name,year),'w') as f:\n",
    "            json.dump(converted_tweets,f)\n",
    "\n",
    "        with open('replies_{0}_{1}.json'.format(screen_name,year),'w') as f:\n",
    "            json.dump(converted_replies,f)\n",
    "        \n",
    "    return converted_tweets, converted_replies\n",
    "\n",
    "def tweet_grabbing_factory(screen_name,min_year=2006,max_year=2018,write_csv=True):\n",
    "    \"\"\"\n",
    "    Given a screen name, get all of their tweets and replies between the min_year and max_year\n",
    "    Returns a DataFrame containing all the tweets in the time range\n",
    "    \"\"\"\n",
    "    \n",
    "    min_year = int(min_year)\n",
    "    max_year = int(max_year)\n",
    "    \n",
    "    tweet_df_list = []\n",
    "    for year in range(min_year,max_year+1):\n",
    "        try:\n",
    "            tweets, replies = tweet_grabber(screen_name,year,False)\n",
    "            \n",
    "            # Convert to DataFrames\n",
    "            tweets_df = pd.DataFrame(tweets)\n",
    "            replies_df = pd.DataFrame(replies)\n",
    "            combined_df = pd.concat([tweets_df,replies_df])\n",
    "            \n",
    "            # Store in the collection for future cleanup\n",
    "            tweet_df_list.append(combined_df)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        \n",
    "        except:\n",
    "            print(\"Error on {0} in {1}\".format(screen_name,year))\n",
    "            pass\n",
    "    \n",
    "    # Concatenate and cleanup all the DataFrames\n",
    "    all_tweets_df = pd.concat(tweet_df_list)\n",
    "    all_tweets_df = all_tweets_df.drop_duplicates(subset=['id'])\n",
    "    # Convert to real timestamps to sort all tweets by time\n",
    "    all_tweets_df['timestamp'] = all_tweets_df['formatted_date'].apply(lambda g: datetime.strptime(g, '%a %b %d %H:%M:%S +%f %Y'))\n",
    "    all_tweets_df = all_tweets_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    if write_csv:\n",
    "    # Write the tweet history to disk\n",
    "        all_tweets_df.to_csv('all_tweets_{0}.csv'.format(screen_name),index=False)\n",
    "    \n",
    "    return all_tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tweets for each candidate. This code is \"commented\" out to prevent it from being inadvertently run because it takes on the order of 2 hours to complete."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "obama_tweets = tweet_grabbing_factory('barackobama')\n",
    "trump_tweets = tweet_grabbing_factory('realdonaldtrump')\n",
    "clinton_tweets = tweet_grabbing_factory('hillaryclinton')\n",
    "romney_tweets = tweet_grabbing_factory('mittromney')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [`python-twitter`](https://python-twitter.readthedocs.io/en/latest/) library and API developer keys, we can \"hydrate\" the tweets from the official Twitter API to get the full payload of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "api = twitter.Api(consumer_key = 'get your own',\n",
    "                  consumer_secret = 'get your own',\n",
    "                  access_token_key = 'get your own',\n",
    "                  access_token_secret = 'get your own',\n",
    "                  tweet_mode = 'extended',\n",
    "                  sleep_on_rate_limit = True\n",
    "                 )\n",
    "\n",
    "def chunks(l, n):\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "def hydrate_tweets(df,column='id'):\n",
    "    tweets = []\n",
    "\n",
    "    for chunk in chunks(df[column].tolist(),100):\n",
    "        _statuses = api.GetStatuses(chunk)\n",
    "        _statuses = [t.AsDict() for t in _statuses]\n",
    "        tweets += _statuses\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrate the tweets for each candidate. This code is \"commented\" out to prevent it from being inadvertently run because it takes on the order of 5 minutes to complete."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "obama_hydrated_tweets = hydrate_tweets(obama_tweets)\n",
    "clinton_hydrated_tweets = hydrate_tweets(clinton_tweets)\n",
    "romney_hydrated_tweets = hydrate_tweets(romney_tweets)\n",
    "trump_hydrated_tweets = hydrate_tweets(trump_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the hydrated data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename,hyrdrated_tweets in zip(['raw_tweets_obama.json','raw_tweets_clinton.json','raw_tweets_romney.json','raw_tweets_trump.json'],[obama_hydrated_tweets,clinton_hydrated_tweets,romney_hydrated_tweets,trump_hydrated_tweets]):\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(hyrdrated_tweets,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_tweets` function will simplify the hydrated tweets into simpler data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet_list):\n",
    "    \n",
    "    cleaned_tweets = []\n",
    "    \n",
    "    for i,t in enumerate(tweet_list):\n",
    "        cleaned_t = {}\n",
    "        cleaned_t['timestamp'] = t.get('created_at',np.nan)\n",
    "        cleaned_t['date'] = datetime.strftime(pd.Timestamp(cleaned_t['timestamp']),'%Y-%m-%d')\n",
    "        cleaned_t['favorites'] = t.get('favorite_count',np.nan)\n",
    "        cleaned_t['text'] = t.get('full_text',np.nan)\n",
    "        cleaned_t['hashtags'] = ', '.join([h.get('text',np.nan) for h in t['hashtags']])\n",
    "        cleaned_t['tweet_id'] = t.get('id_str',np.nan)\n",
    "        cleaned_t['retweets'] = t.get('retweet_count',np.nan)\n",
    "        cleaned_t['screen_name'] = t['user'].get('screen_name',np.nan)\n",
    "        cleaned_t['mentions'] = ', '.join([u.get('screen_name',np.nan) for u in t['user_mentions']])\n",
    "        cleaned_t['source'] = BeautifulSoup(t.get('source','')).text\n",
    "        \n",
    "        # This is where the sentiment analysis magic happens\n",
    "        cleaned_t['sentiment'] = analyzer.polarity_scores(cleaned_t['text'])['compound']\n",
    "        cleaned_tweets.append(cleaned_t)\n",
    "        \n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `clean_tweets` functions to the hydrated tweets for each candidate. Write the cleaned data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_cleaned_tweets = clean_tweets(obama_hydrated_tweets)\n",
    "clinton_cleaned_tweets = clean_tweets(clinton_hydrated_tweets)\n",
    "romney_cleaned_tweets = clean_tweets(romney_hydrated_tweets)\n",
    "trump_cleaned_tweets = clean_tweets(trump_hydrated_tweets)\n",
    "\n",
    "for filename,cleaned_tweets in zip(['cleaned_tweets_obama.json','cleaned_tweets_clinton.json','cleaned_tweets_romney.json','cleaned_tweets_trump.json'],[obama_cleaned_tweets,clinton_cleaned_tweets,romney_cleaned_tweets,trump_cleaned_tweets]):\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(cleaned_tweets,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
